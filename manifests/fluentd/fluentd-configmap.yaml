kind: ConfigMap
apiVersion: v1
data:
  containers.input.conf: |-
    # Example:
    # {"log":"[info:2016-02-16T16:04:05.930-08:00] Some log text here\n","stream":"stdout","time":"2016-02-17T00:04:05.931087621Z"}
    <source>
      type tail
      path /var/log/containers/*.log
      pos_file /var/log/containers.log.pos
      time_format %Y-%m-%dT%H:%M:%S.%NZ
      tag kubernetes.*
      format json
      keep_time_key true
      read_from_head true
    </source>
  system.input.conf: |-
    # Examples:
    # time="2016-02-04T06:51:03.053580605Z" level=info msg="GET /containers/json"
    # time="2016-02-04T07:53:57.505612354Z" level=error msg="HTTP Error" err="No such image: -f" statusCode=404
    <source>
      type tail
      format /^time="(?<time>[^)]*)" level=(?<severity>[^ ]*) msg="(?<message>[^"]*)"( err="(?<error>[^"]*)")?( statusCode=($<status_code>\d+))?/
      path /var/log/docker.log
      pos_file /var/log/docker.log.pos
      tag docker
    </source>

    # Multi-line parsing is required for all the kube logs because very large log
    # statements, such as those that include entire object bodies, get split into
    # multiple lines by glog.

    # Example:
    # I0204 07:32:30.020537    3368 server.go:1048] POST /stats/container/: (13.972191ms) 200 [[Go-http-client/1.1] 10.244.1.3:40537]
    <source>
      type tail
      format multiline
      multiline_flush_interval 5s
      format_firstline /^\w\d{4}/
      format1 /^(?<severity>\w)(?<time>\d{4} [^\s]*)\s+(?<pid>\d+)\s+(?<source>[^ \]]+)\] (?<message>.*)/
      time_format %m%d %H:%M:%S.%N
      path /var/log/kubelet.log
      pos_file /var/log/kubelet.log.pos
      tag kubelet
    </source>

    # Logs from systemd-journal for interesting services.
    <source>
      type systemd
      filters [{ "_SYSTEMD_UNIT": "docker.service" }]
      pos_file /var/log/gcp-journald-docker.pos
      read_from_head true
      tag docker
    </source>

    <source>
      type systemd
      filters [{ "_SYSTEMD_UNIT": "kubelet.service" }]
      pos_file /var/log/gcp-journald-kubelet.pos
      read_from_head true
      tag kubelet
    </source>

  forward.input.conf: |-
    # Takes the messages sent over TCP
    <source>
      type forward
    </source>
  monitoring.conf: |-
    # Prometheus Exporter Plugin
    # input plugin that exports metrics
    <source>
      @type prometheus
    </source>

    <source>
      @type monitor_agent
    </source>

    # input plugin that collects metrics from MonitorAgent
    <source>
      @type prometheus_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # input plugin that collects metrics for output plugin
    <source>
      @type prometheus_output_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>

    # input plugin that collects metrics for in_tail plugin
    <source>
      @type prometheus_tail_monitor
      <labels>
        host ${hostname}
      </labels>
    </source>
  output.conf: |-
    # Enriches records with Kubernetes metadata
    <filter kubernetes.**>
      type kubernetes_metadata
      merge_json_log false
    </filter>

    <filter kubernetes.**>
      @type record_modifier
      <record>
        _json_log_             ${ log = record["log"].strip; if log[0].eql?('{') && log[-1].eql?('}'); begin; JSON.parse(log); rescue JSON::ParserError; end; end }
        timestamp              ${time}
        nsec                   ${record["time"].split('.').last.to_i}
        namespace              ${record["kubernetes"]["namespace_name"]}
        labels.key             ${record["kubernetes"]["labels"].keys}
        labels.value           ${record["kubernetes"]["labels"].values}
        host                   ${record["kubernetes"]["host"]}
        pod_name               ${record["kubernetes"]["pod_name"]}
        container_name         ${record["kubernetes"]["container_name"]}
        logs.key               ${record["_json_log_"] ? record["_json_log_"].keys : ["log"]}
        logs.value             ${record["_json_log_"] ? record["_json_log_"].values.map(&:to_s) : [record["log"]]}
      </record>
      whitelist_keys           tag
    </filter>

    <filter docker.**>
      @type record_modifier
      <record>
        timestamp              ${time}
        nsec                   ${record["time"].split('.'.last.to_i}
        host                   "#{Socket.gethostname}"
        logs.key               ${["level", "msg", "err", "statusCode"]}
        logs.value             ${[record["level"], record["msg"], record["err"], record["statusCode"]]}
      </record>
      whitelist_keys           tag
    </filter>

    <filter kubelet.**>
      @type record_modifier
      <record>
        timestamp              ${time}
        nsec                   ${record["time"].split('.'.last.to_i}
        host                   "#{Socket.gethostname}"
        logs.key               ${["_TRANSPORT", "PRIORITY", "SYSLOG_FACILITY", "SYSLOG_IDENTIFIER", "_PID", "_UID", "_GID", "_COMM", "_EXE", "_CMDLINE", "_CAP_EFFECTIVE", "_SYSTEMD_CGROUP", "_SYSTEMD_UNIT", "_SYSTEMD_SLICE", "_BOOT_ID", "_MACHINE_ID", "_HOSTNAME", "MESSAGE"]}
        logs.value             ${[record["_TRANSPORT"], record["PRIORITY"], record["SYSLOG_FACILITY"], record["SYSLOG_IDENTIFIER"], record["_PID"], record["_UID"], record["_GID"], record["_COMM"], record["_EXE"], record["_CMDLINE"], record["_CAP_EFFECTIVE"], record["_SYSTEMD_CGROUP"], record["_SYSTEMD_UNIT"], record["_SYSTEMD_SLICE"], record["_BOOT_ID"], record["_MACHINE_ID"], record["_HOSTNAME"], record["MESSAGE"]]}
      </record>
      whitelist_keys           tag
    </filter>

    <match **>
      @type exec
      command bash /usr/local/bin/insert_ch.sh
      format json
      buffer_type memory
      buffer_chunk_limit 32m
      buffer_queue_limit 32
      flush_interval 1s
      num_threads 4
    </match>
metadata:
  name: fluentd-config-v0.1.0
  namespace: kube-logging
  labels:
    addonmanager.kubernetes.io/mode: Reconcile
